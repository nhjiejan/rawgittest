{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from url_parsing import simplify_url\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from train_utils import build_stream_df, build_train_test_data\n",
    "\n",
    "from recommender import Recommender\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "The first step in data preparation is to convert a dataframe of individual clicks to a dataframe with lists of sequential clicks by user. This dataframe will be called 'stream_df'. The urls are then encoded to numerical indices and then into one-hot representations.\n",
    "\n",
    "One of the main challenges with making recommendations for the intent of an online customer is the lack of labelled data. As a proxy of the intent we will use clicks on FAQ pages. Clearly, if a user accessed a page \"how to change my billing address\" then the intent behind the online journey is likely to change the billing address.\n",
    "\n",
    "Most observed journeys do not contain clicks on FAQ pages. In the following we will downsample the dataset to only include journeys containing such pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data/click_call_balanced.csv'\n",
    "UNIQUE_ID = 'UCRN'\n",
    "n_datapoints = None\n",
    "\n",
    "df = pd.read_csv(data_file, sep=',')\n",
    "df['date_time'] = df['date_time'].apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "df = df[['pagename', 'date_time', UNIQUE_ID, 'has_call']]\n",
    "\n",
    "simple_urls = [simplify_url(page) for page in df['pagename']]\n",
    "df.loc[:, 'pagename'] = simple_urls\n",
    "\n",
    "stream_df = build_stream_df(df, UNIQUE_ID)\n",
    "\n",
    "all_urls = np.unique(np.concatenate(stream_df['url_sequence'].values))\n",
    "l_enc = LabelEncoder()    \n",
    "enc_urls = l_enc.fit(all_urls)\n",
    "\n",
    "stream_df['encoded_sequence'] = [l_enc.transform(x[:-1]) for x in stream_df['url_sequence']]\n",
    "\n",
    "oh_enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "oh_enc.fit(np.sort(np.unique(np.concatenate(stream_df['encoded_sequence'].values))).reshape(-1, 1))\n",
    "\n",
    "stream_df['oh_encoded_sequence'] = [oh_enc.transform(x.reshape(-1, 1)) for x in stream_df['encoded_sequence']]\n",
    "\n",
    "journey = stream_df['url_sequence'].values[0]\n",
    "rec = Recommender()\n",
    "recs = np.array([rec.recommend(journey) for journey in stream_df['url_sequence'].values])\n",
    "keep_idxs = np.where((recs!='unknown') & (recs!='cycle longer than 40 clicks') & (recs!='over 40 clicks'))[0]\n",
    "\n",
    "observations = stream_df['oh_encoded_sequence'].iloc[keep_idxs].values\n",
    "journeys = np.array([np.sum(obs,axis=0) for obs in observations])\n",
    "labels = recs[keep_idxs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMMLearn Approach\n",
    "In this approach we will use a Hidden Markov model where the hidden state is the thought to be the intent and the observations are the encoded urls.\n",
    "We conduct experiments with Gaussian emission probabilities and multinomial emission probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianHMM(algorithm='viterbi', covariance_type='diag', covars_prior=0.01,\n",
       "      covars_weight=1, init_params='se', means_prior=0, means_weight=0,\n",
       "      min_covar=0.001, n_components=10, n_iter=1000, params='se',\n",
       "      random_state=None, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
       "      verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hmmlearn import hmm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get a dense representation of the observed journeys using SVD for the Gaussian HMM\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(journeys)\n",
    "observations_vecs = [svd.transform(obs) for obs in observations]\n",
    "\n",
    "# encode the FAQ labels\n",
    "intent_enc = LabelEncoder()\n",
    "enc_labels = intent_enc.fit_transform(labels)\n",
    "\n",
    "# idea 1: use a HMM with gaussian emmissions on the dense observations\n",
    "observations_vecs_train, observations_vecs_test, vecs_y_train, vecs_y_test = train_test_split(\n",
    "    observations_vecs, enc_labels, test_size=0.33, random_state=42)\n",
    "\n",
    "vecs_lengths_train = [len(obs) for obs in observations_vecs_train]\n",
    "vecs_lengths_test = [len(obs) for obs in observations_vecs_test]\n",
    "\n",
    "gaussian_model = hmm.GaussianHMM(n_components=10, covariance_type=\"diag\", n_iter=1000, params='se', init_params='se')\n",
    "gaussian_model.transmat_ = np.diag(np.ones(n_components))\n",
    "gaussian_model.fit(np.concatenate(observations_vecs_train), vecs_lengths_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -378360.7910             +nan\n",
      "         2     -244626.2556     +133734.5355\n",
      "         3     -241724.0121       +2902.2435\n",
      "         4     -240685.2459       +1038.7662\n",
      "         5     -240258.2226        +427.0233\n",
      "         6     -240010.5880        +247.6346\n",
      "         7     -239909.1195        +101.4685\n",
      "         8     -239842.1626         +66.9569\n",
      "         9     -239804.3763         +37.7863\n",
      "        10     -239775.4390         +28.9372\n",
      "        11     -239739.8892         +35.5498\n",
      "        12     -239733.2165          +6.6728\n",
      "        13     -239725.8385          +7.3779\n",
      "        14     -239722.9506          +2.8879\n",
      "        15     -239686.9594         +35.9913\n",
      "        16     -239663.1820         +23.7774\n",
      "        17     -239646.2716         +16.9103\n",
      "        18     -239642.6430          +3.6286\n",
      "        19     -239638.8998          +3.7432\n",
      "        20     -239633.6328          +5.2670\n",
      "        21     -239631.6068          +2.0260\n",
      "        22     -239630.1493          +1.4575\n",
      "        23     -239630.0719          +0.0774\n",
      "        24     -239630.0606          +0.0113\n",
      "        25     -239630.0515          +0.0091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialHMM(algorithm='viterbi', init_params='se', n_components=10,\n",
       "        n_iter=100, params='se',\n",
       "        random_state=<mtrand.RandomState object at 0x107e736e0>,\n",
       "        startprob_prior=1.0, tol=0.01,\n",
       "        transmat_prior=array([[ 1.,  0., ...,  0.,  0.],\n",
       "       [ 0.,  1., ...,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0., ...,  1.,  0.],\n",
       "       [ 0.,  0., ...,  0.,  1.]]),\n",
       "        verbose=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idea 2: use a HMM with multinomial emissions on the one-hot encoded observations\n",
    "n_components = 10\n",
    "observations_resh = [o[:, np.newaxis] for o in stream_df['encoded_sequence'][keep_idxs].values]\n",
    "\n",
    "transmat_prior = np.diag(np.ones(n_components))    \n",
    "multinomial_model = hmm.MultinomialHMM(n_components=n_components, n_iter=100, transmat_prior=transmat_prior, \n",
    "                           verbose=True, params='se', init_params='se')\n",
    "n_features = np.max(np.concatenate(observations_resh))\n",
    "start_probability = np.ones(n_features)/float(n_features)\n",
    "\n",
    "# no transitions of the state\n",
    "transition_probability = np.diag(np.ones(n_components))\n",
    "# uniform prior\n",
    "emission_probability = np.ones((n_components, n_features))/n_features\n",
    "\n",
    "#gaussian_model.startprob_ = start_probability\n",
    "multinomial_model.transmat_ = transition_probability\n",
    "\n",
    "# when fitting the HMM we need at least one observation of every state\n",
    "#reversed_observations = [obs[::-1] for obs in observations]\n",
    "X_train, X_test, y_train, y_test = train_test_split(observations_resh, enc_labels, test_size=0.33, random_state=42)\n",
    "\n",
    "train_lengths = [len(obs) for obs in X_train]\n",
    "# have to add this to avoid exception\n",
    "dummy = np.arange(len(l_enc.classes_))[:,np.newaxis]\n",
    "train_lengths += [len(dummy)]\n",
    "multinomial_model.fit(np.concatenate([np.concatenate(X_train), dummy]), train_lengths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian HMM Train Acc: 0.00822078684674\n",
      "Gaussian HMM Test Acc: 0.00822078684674\n",
      "Multinomial HMM Train Acc: 0.00788781770377\n",
      "Multinomial HMM Test Acc: 0.00711743772242\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, preds):\n",
    "    score=0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] in preds[i]:\n",
    "            score+=1\n",
    "    score /= float(len(preds))\n",
    "    return score\n",
    "\n",
    "preds_train_gaussian = [gaussian_model.predict(obs) for obs in observations_vecs]\n",
    "preds_test_gaussian = [gaussian_model.predict(obs) for obs in observations_vecs]\n",
    "\n",
    "preds_train_multi = [multinomial_model.predict(obs)[-1] for i,obs in enumerate(X_train)]\n",
    "preds_test_multi = [multinomial_model.predict(obs)[-1] for i,obs in enumerate(X_test)]\n",
    "\n",
    "print(\"Gaussian HMM Train Acc: {}\").format(accuracy(y_train, [x for x in preds_train_gaussian]))\n",
    "print(\"Gaussian HMM Test Acc: {}\").format(accuracy(y_test, [x for x in preds_test_gaussian]))\n",
    "print(\"Multinomial HMM Train Acc: {}\").format(accuracy(y_train, [[x] for x in preds_train_multi]))\n",
    "print(\"Multinomial HMM Test Acc: {}\").format(accuracy(y_test, [[x] for x in preds_test_multi]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Bayes Rule\n",
    "This is an extension of the HMM. What we can do is use a simple HMM with few hidden states (e.g. 2) and then use Bayes Rule to infer the FAQ given the hidden state. This way many different journeys can be represented as the same state giving a higher level representation. In the following 'intent' will denote the hidden state of the HMM and 'faq' will denote the prediction of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer probability table for p(faq|intent) using bayes rule\n",
    "faqs = np.unique(enc_labels)\n",
    "n_faqs = len(faqs)\n",
    "prob_matrix = np.zeros((n_faqs, n_components))\n",
    "for faq,intent in zip(enc_labels, preds_train_multi):\n",
    "    prob_matrix[faq,intent] += 1\n",
    "prob_matrix = prob_matrix / np.sum(prob_matrix, axis=1)[:, np.newaxis]\n",
    "prob_matrix[np.sum(np.isnan(prob_matrix), axis=1)>1] = 1./prob_matrix.shape[1]\n",
    "p_faq = Counter(y_train)\n",
    "for k in p_faq.keys():\n",
    "    p_faq[k] /= float(len(y_train))\n",
    "\n",
    "# P(intent)\n",
    "p_intent = Counter(preds_train)\n",
    "for k in p_intent.keys():\n",
    "    p_intent[k] /= float(len(preds_train))\n",
    "\n",
    "# P(FAQ | intent)\n",
    "def p_faq_intent(faq, intent):\n",
    "    return prob_matrix[faq,intent] * p_faq[faq] / p_intent[intent]\n",
    "\n",
    "# P(FAQ)\n",
    "def get_faq(intent, top_n=1):\n",
    "    return np.argsort([p_faq_intent(f, intent) for f in faqs])[-top_n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayes Rule Train Acc: 0.336546888694\n",
      "Bayes Rule Test Acc: 0.377224199288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['how can you help me with my gas or boiler annual service?']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Bayes Rule Train Acc: {}\").format(accuracy(y_train, [get_faq(pred, 3) for pred in preds_train]))\n",
    "print(\"Bayes Rule Test Acc: {}\").format(accuracy(y_test, [get_faq(pred, 3) for pred in preds_test]))\n",
    "#np.sum(enc_labels==pred_faq)\n",
    "\n",
    "# do a few tests\n",
    "idx = 0\n",
    "label = labels[0]\n",
    "journey = stream_df['url_sequence'].iloc[keep_idxs].values[idx]\n",
    "enc_journey = stream_df['encoded_sequence'].iloc[keep_idxs].values[idx]\n",
    "pred = multinomial_model.predict(enc_journey.reshape(-1,1))[-1]\n",
    "faq = get_faq(pred)\n",
    "[intent_enc.inverse_transform(f) for f in faq]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
